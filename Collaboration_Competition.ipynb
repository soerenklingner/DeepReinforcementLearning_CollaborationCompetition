{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "LEARN_EVERY = 1         # learning timestep interval\n",
    "LEARN_NUM = 1           # number of learning passes\n",
    "GAMMA = 0.99            # discount factor\n",
    "\n",
    "TAU = 7e-2              # for soft update of target parameters\n",
    "OU_SIGMA = 0.2          # Ornstein-Uhlenbeck noise parameter, volatility\n",
    "OU_THETA = 0.12         # Ornstein-Uhlenbeck noise parameter, speed of mean reversion\n",
    "EPS_START = 5.5         # initial value for epsilon in noise decay process in Agent.act()\n",
    "EPS_EP_END = 250        # episode to end the noise decay process\n",
    "EPS_FINAL = 0           # final value for epsilon after decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Learns and interacts with the environment\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            num_agents (int): number of agents\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.eps = EPS_START\n",
    "        self.eps_decay = 1/(EPS_EP_END*LEARN_NUM)  # set decay rate based on epsilon end target\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, agent_number):\n",
    "        \"\"\"Save rewards in replay memory and use random sample from buffer for learning.\"\"\"\n",
    "        \n",
    "        self.timestep += 1\n",
    "        # Save reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn, if enough samples are available in memory and at learning interval steps\n",
    "        if len(self.memory) > BATCH_SIZE and self.timestep % LEARN_EVERY == 0:\n",
    "                for _ in range(LEARN_NUM):\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, GAMMA, agent_number)\n",
    "\n",
    "    def act(self, states, add_noise):\n",
    "        \"\"\"Returns actions for both agents given their current policy and states.\"\"\"\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.zeros((self.num_agents, self.action_size))\n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get action for each agent and concatenate them\n",
    "            for agent_num, state in enumerate(states):\n",
    "                action = self.actor_local(state).cpu().data.numpy()\n",
    "                actions[agent_num, :] = action\n",
    "        \n",
    "        self.actor_local.train()\n",
    "        \n",
    "        # add noise to the actions\n",
    "        if add_noise:\n",
    "            actions += self.eps * self.noise.sample()\n",
    "        \n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma, agent_number):\n",
    "        \"\"\"Update policy and value parameters.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        \n",
    "        # Construct next actions vector relative to the agent\n",
    "        if agent_number == 0:\n",
    "            actions_next = torch.cat((actions_next, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_next = torch.cat((actions[:,:2], actions_next), dim=1)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        \n",
    "        # Construct action prediction vector relative to each agent\n",
    "        if agent_number == 0:\n",
    "            actions_pred = torch.cat((actions_pred, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_pred = torch.cat((actions[:,:2], actions_pred), dim=1)\n",
    "        \n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "        # update noise decay parameter\n",
    "        self.eps -= self.eps_decay\n",
    "        self.eps = max(self.eps, EPS_FINAL)\n",
    "        self.noise.reset()\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck noise.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0.0, theta=OU_THETA, sigma=OU_SIGMA):\n",
    "        \"\"\"Initialize\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            mu (float)    : long-running mean\n",
    "            theta (float) : speed of mean reversion\n",
    "            sigma (float) : volatility parameter\n",
    "        \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update and returns a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new rewatd/experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size*2, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size*2, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+(action_size*2), fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSEC_EPISODES = 100\n",
    "PRINT_EVERY = 10\n",
    "ADD_NOISE = True\n",
    "\n",
    "# MADDPG function\n",
    "\n",
    "def maddpg(n_episodes=10000, max_t=1000, train_mode=True):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int)      : maximum number of training episodes\n",
    "        max_t (int)           : maximum number of timesteps per episode\n",
    "        train_mode (bool)     : if 'True' set environment to training mode\n",
    "\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=CONSEC_EPISODES)\n",
    "    scores_all = []\n",
    "    average = []\n",
    "    best_score = -np.inf\n",
    "    best_episode = 0\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]         # reset the environment\n",
    "        states = np.reshape(env_info.vector_observations, (1,48)) # get states and combine them\n",
    "        \n",
    "        agent_a.reset()\n",
    "        agent_b.reset()\n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        while True:            \n",
    "            # actions for each agent and then combines them into one array\n",
    "            action_a = agent_a.act(states, ADD_NOISE)\n",
    "            action_b = agent_b.act(states, ADD_NOISE)\n",
    "            actions = np.concatenate((action_a, action_b), axis=0).flatten()\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]           # send both agents' actions together to the environment\n",
    "            next_states = np.reshape(env_info.vector_observations, (1, 48)) # combine the agent next states\n",
    "            rewards = env_info.rewards                         # get reward\n",
    "            done = env_info.local_done                         # see if episode finished\n",
    "            \n",
    "            agent_a.step(states, actions, rewards[0], next_states, done, 0) # agent 1 learns\n",
    "            agent_b.step(states, actions, rewards[1], next_states, done, 1) # agent 2 learns\n",
    "            \n",
    "            scores += np.max(rewards)                          # update the score for each agent\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(done):                                   # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        ep_best_score = np.max(scores)\n",
    "        scores_window.append(ep_best_score)\n",
    "        scores_all.append(ep_best_score)\n",
    "        average.append(np.mean(scores_window))\n",
    "\n",
    "        # save best score                        \n",
    "        if ep_best_score > best_score:\n",
    "            best_score = ep_best_score\n",
    "            best_episode = i_episode\n",
    "        \n",
    "        # print results\n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            print('Episode {:d}\\tMax: {:f}\\tAverage: {:f}'.format(i_episode, np.max(scores_all[-PRINT_EVERY:]), average[-1]))\n",
    "\n",
    "        # the goal is a score of +0.5\n",
    "        if average[-1] >= 0.5:        \n",
    "            print('Environment solved in {:d} episodes. Average: {:f} over past {:d} episodes'.format(\n",
    "                i_episode-CONSEC_EPISODES, average[-1], CONSEC_EPISODES))\n",
    "\n",
    "            # save weights\n",
    "            torch.save(agent_a.actor_local.state_dict(), 'saved_model_actor_a.pth')\n",
    "            torch.save(agent_a.critic_local.state_dict(), 'saved_model_critic_a.pth')\n",
    "            torch.save(agent_b.actor_local.state_dict(), 'saved_model_actor_b.pth')\n",
    "            torch.save(agent_b.critic_local.state_dict(), 'saved_model_critic_b.pth')\n",
    "            \n",
    "            break\n",
    "\n",
    "    return scores_all, average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agents\n",
    "agent_a = Agent(state_size, action_size, num_agents=1, random_seed=0)\n",
    "agent_b = Agent(state_size, action_size, num_agents=1, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tMax: 0.400000\tAverage: 0.190000\n",
      "Episode 20\tMax: 0.600000\tAverage: 0.240000\n",
      "Episode 30\tMax: 0.600000\tAverage: 0.266667\n",
      "Episode 40\tMax: 0.700000\tAverage: 0.275000\n",
      "Episode 50\tMax: 0.800000\tAverage: 0.278000\n",
      "Episode 60\tMax: 0.600000\tAverage: 0.286667\n",
      "Episode 70\tMax: 0.600000\tAverage: 0.282857\n",
      "Episode 80\tMax: 0.600000\tAverage: 0.291250\n",
      "Episode 90\tMax: 0.600000\tAverage: 0.294444\n",
      "Episode 100\tMax: 1.000000\tAverage: 0.311000\n",
      "Episode 110\tMax: 0.600000\tAverage: 0.320000\n",
      "Episode 120\tMax: 0.400000\tAverage: 0.318000\n",
      "Episode 130\tMax: 0.600000\tAverage: 0.314000\n",
      "Episode 140\tMax: 0.500000\tAverage: 0.310000\n",
      "Episode 150\tMax: 0.500000\tAverage: 0.305000\n",
      "Episode 160\tMax: 0.600000\tAverage: 0.304000\n",
      "Episode 170\tMax: 0.400000\tAverage: 0.303000\n",
      "Episode 180\tMax: 0.400000\tAverage: 0.289000\n",
      "Episode 190\tMax: 0.400000\tAverage: 0.273000\n",
      "Episode 200\tMax: 0.500000\tAverage: 0.254000\n",
      "Episode 210\tMax: 0.400000\tAverage: 0.251000\n",
      "Episode 220\tMax: 0.600000\tAverage: 0.259000\n",
      "Episode 230\tMax: 0.500000\tAverage: 0.257000\n",
      "Episode 240\tMax: 0.400000\tAverage: 0.258000\n",
      "Episode 250\tMax: 0.400000\tAverage: 0.256000\n",
      "Episode 260\tMax: 0.400000\tAverage: 0.244000\n",
      "Episode 270\tMax: 0.600000\tAverage: 0.248000\n",
      "Episode 280\tMax: 0.400000\tAverage: 0.254000\n",
      "Episode 290\tMax: 0.600000\tAverage: 0.261000\n",
      "Episode 300\tMax: 0.500000\tAverage: 0.252900\n",
      "Episode 310\tMax: 0.600000\tAverage: 0.260900\n",
      "Episode 320\tMax: 0.400000\tAverage: 0.251900\n",
      "Episode 330\tMax: 0.400000\tAverage: 0.255900\n",
      "Episode 340\tMax: 0.800000\tAverage: 0.266900\n",
      "Episode 350\tMax: 1.100000\tAverage: 0.279900\n",
      "Episode 360\tMax: 0.800000\tAverage: 0.292900\n",
      "Episode 370\tMax: 0.600000\tAverage: 0.299900\n",
      "Episode 380\tMax: 0.600000\tAverage: 0.306900\n",
      "Episode 390\tMax: 0.600000\tAverage: 0.307900\n",
      "Episode 400\tMax: 0.800000\tAverage: 0.322000\n",
      "Episode 410\tMax: 0.800000\tAverage: 0.325000\n",
      "Episode 420\tMax: 0.400000\tAverage: 0.331000\n",
      "Episode 430\tMax: 0.700000\tAverage: 0.335000\n",
      "Episode 440\tMax: 0.600000\tAverage: 0.334000\n",
      "Episode 450\tMax: 0.400000\tAverage: 0.325000\n",
      "Episode 460\tMax: 0.800000\tAverage: 0.336000\n",
      "Episode 470\tMax: 0.800000\tAverage: 0.335000\n",
      "Episode 480\tMax: 0.700000\tAverage: 0.330000\n",
      "Episode 490\tMax: 0.600000\tAverage: 0.334000\n",
      "Episode 500\tMax: 0.400000\tAverage: 0.327000\n",
      "Episode 510\tMax: 0.800000\tAverage: 0.332000\n",
      "Episode 520\tMax: 0.600000\tAverage: 0.333000\n",
      "Episode 530\tMax: 0.500000\tAverage: 0.325000\n",
      "Episode 540\tMax: 0.700000\tAverage: 0.319000\n",
      "Episode 550\tMax: 0.800000\tAverage: 0.319000\n",
      "Episode 560\tMax: 0.600000\tAverage: 0.307000\n",
      "Episode 570\tMax: 0.400000\tAverage: 0.301000\n",
      "Episode 580\tMax: 0.400000\tAverage: 0.296000\n",
      "Episode 590\tMax: 1.000000\tAverage: 0.305000\n",
      "Episode 600\tMax: 0.400000\tAverage: 0.305000\n",
      "Episode 610\tMax: 0.400000\tAverage: 0.289000\n",
      "Episode 620\tMax: 0.600000\tAverage: 0.286000\n",
      "Episode 630\tMax: 0.600000\tAverage: 0.295000\n",
      "Episode 640\tMax: 0.400000\tAverage: 0.280000\n",
      "Episode 650\tMax: 0.700000\tAverage: 0.289000\n",
      "Episode 660\tMax: 0.700000\tAverage: 0.297000\n",
      "Episode 670\tMax: 0.400000\tAverage: 0.295000\n",
      "Episode 680\tMax: 0.800000\tAverage: 0.307000\n",
      "Episode 690\tMax: 0.600000\tAverage: 0.299000\n",
      "Episode 700\tMax: 0.600000\tAverage: 0.303000\n",
      "Episode 710\tMax: 0.600000\tAverage: 0.303000\n",
      "Episode 720\tMax: 0.800000\tAverage: 0.314000\n",
      "Episode 730\tMax: 0.400000\tAverage: 0.306000\n",
      "Episode 740\tMax: 0.400000\tAverage: 0.316900\n",
      "Episode 750\tMax: 0.400000\tAverage: 0.303900\n",
      "Episode 760\tMax: 0.700000\tAverage: 0.299900\n",
      "Episode 770\tMax: 1.000000\tAverage: 0.314900\n",
      "Episode 780\tMax: 0.500000\tAverage: 0.304900\n",
      "Episode 790\tMax: 0.600000\tAverage: 0.298900\n",
      "Episode 800\tMax: 0.600000\tAverage: 0.308900\n",
      "Episode 810\tMax: 0.600000\tAverage: 0.321900\n",
      "Episode 820\tMax: 1.200000\tAverage: 0.337900\n",
      "Episode 830\tMax: 1.200000\tAverage: 0.360900\n",
      "Episode 840\tMax: 0.500000\tAverage: 0.372000\n",
      "Episode 850\tMax: 1.000000\tAverage: 0.395000\n",
      "Episode 860\tMax: 0.800000\tAverage: 0.399000\n",
      "Episode 870\tMax: 1.000000\tAverage: 0.408000\n",
      "Episode 880\tMax: 0.600000\tAverage: 0.415000\n",
      "Episode 890\tMax: 0.600000\tAverage: 0.432000\n",
      "Episode 900\tMax: 0.500000\tAverage: 0.429000\n",
      "Episode 910\tMax: 0.400000\tAverage: 0.426000\n",
      "Episode 920\tMax: 0.700000\tAverage: 0.408000\n",
      "Episode 930\tMax: 0.900000\tAverage: 0.393000\n",
      "Episode 940\tMax: 0.600000\tAverage: 0.390000\n",
      "Episode 950\tMax: 0.800000\tAverage: 0.388000\n",
      "Episode 960\tMax: 0.400000\tAverage: 0.382000\n",
      "Episode 970\tMax: 0.900000\tAverage: 0.365000\n",
      "Episode 980\tMax: 0.500000\tAverage: 0.368000\n",
      "Episode 990\tMax: 2.100000\tAverage: 0.379000\n",
      "Episode 1000\tMax: 0.700000\tAverage: 0.382000\n",
      "Episode 1010\tMax: 0.600000\tAverage: 0.388000\n",
      "Episode 1020\tMax: 0.600000\tAverage: 0.381000\n",
      "Episode 1030\tMax: 0.400000\tAverage: 0.364000\n",
      "Episode 1040\tMax: 0.900000\tAverage: 0.368000\n",
      "Episode 1050\tMax: 0.600000\tAverage: 0.365000\n",
      "Episode 1060\tMax: 0.700000\tAverage: 0.370000\n",
      "Episode 1070\tMax: 0.700000\tAverage: 0.374000\n",
      "Episode 1080\tMax: 0.700000\tAverage: 0.380000\n",
      "Episode 1090\tMax: 1.200000\tAverage: 0.369000\n",
      "Episode 1100\tMax: 0.500000\tAverage: 0.364000\n",
      "Episode 1110\tMax: 0.400000\tAverage: 0.355000\n",
      "Episode 1120\tMax: 0.900000\tAverage: 0.361000\n",
      "Episode 1130\tMax: 0.500000\tAverage: 0.381000\n",
      "Episode 1140\tMax: 0.600000\tAverage: 0.372000\n",
      "Episode 1150\tMax: 0.500000\tAverage: 0.366000\n",
      "Episode 1160\tMax: 0.500000\tAverage: 0.354000\n",
      "Episode 1170\tMax: 0.400000\tAverage: 0.342000\n",
      "Episode 1180\tMax: 0.400000\tAverage: 0.331000\n",
      "Episode 1190\tMax: 0.500000\tAverage: 0.319000\n",
      "Episode 1200\tMax: 0.700000\tAverage: 0.321000\n",
      "Episode 1210\tMax: 0.600000\tAverage: 0.326000\n",
      "Episode 1220\tMax: 0.600000\tAverage: 0.318000\n",
      "Episode 1230\tMax: 1.700000\tAverage: 0.340000\n",
      "Episode 1240\tMax: 0.900000\tAverage: 0.354000\n",
      "Episode 1250\tMax: 1.100000\tAverage: 0.360000\n",
      "Episode 1260\tMax: 0.400000\tAverage: 0.361000\n",
      "Episode 1270\tMax: 0.500000\tAverage: 0.371000\n",
      "Episode 1280\tMax: 0.600000\tAverage: 0.374000\n",
      "Episode 1290\tMax: 0.800000\tAverage: 0.381000\n",
      "Episode 1300\tMax: 0.600000\tAverage: 0.382000\n",
      "Episode 1310\tMax: 0.500000\tAverage: 0.384000\n",
      "Episode 1320\tMax: 0.600000\tAverage: 0.386000\n",
      "Episode 1330\tMax: 0.500000\tAverage: 0.364000\n",
      "Episode 1340\tMax: 0.700000\tAverage: 0.360000\n",
      "Episode 1350\tMax: 0.600000\tAverage: 0.351000\n",
      "Episode 1360\tMax: 0.600000\tAverage: 0.354000\n",
      "Episode 1370\tMax: 0.600000\tAverage: 0.360000\n",
      "Episode 1380\tMax: 1.600000\tAverage: 0.377000\n",
      "Episode 1390\tMax: 0.700000\tAverage: 0.377000\n",
      "Episode 1400\tMax: 0.800000\tAverage: 0.388000\n",
      "Episode 1410\tMax: 0.500000\tAverage: 0.388900\n",
      "Episode 1420\tMax: 1.700000\tAverage: 0.414900\n",
      "Episode 1430\tMax: 2.200000\tAverage: 0.455900\n",
      "Episode 1440\tMax: 0.700000\tAverage: 0.457900\n",
      "Episode 1450\tMax: 0.700000\tAverage: 0.471800\n",
      "Episode 1460\tMax: 0.600000\tAverage: 0.486800\n",
      "Episode 1470\tMax: 0.600000\tAverage: 0.483800\n",
      "Environment solved in 1375 episodes. Average: 0.503800 over past 100 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores, avgs = maddpg()\n",
    "\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='MADDPG')\n",
    "plt.plot(np.arange(len(scores)), avgs, c='r', label='avg')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
